{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Architecture.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuvaraj0001/EVA7_Assignments/blob/main/Session%204/PART%202/Neural_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej2-Ahv-vJ8D"
      },
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C6nsqETvMHz"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC8xOfnfvjnp"
      },
      "source": [
        "## Building the Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yuN9v1JyP-h"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.avg_pool = nn.Sequential(\n",
        "            nn.AvgPool2d(1, stride=1, padding=0)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "                \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e11c8a5-5d99-4e16-f619-2ecf1606eebf"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model=Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "              ReLU-2           [-1, 32, 28, 28]               0\n",
            "       BatchNorm2d-3           [-1, 32, 28, 28]              64\n",
            "            Conv2d-4           [-1, 32, 28, 28]           9,248\n",
            "              ReLU-5           [-1, 32, 28, 28]               0\n",
            "       BatchNorm2d-6           [-1, 32, 28, 28]              64\n",
            "            Conv2d-7           [-1, 32, 14, 14]           9,248\n",
            "              ReLU-8           [-1, 32, 14, 14]               0\n",
            "       BatchNorm2d-9           [-1, 32, 14, 14]              64\n",
            "        MaxPool2d-10             [-1, 32, 7, 7]               0\n",
            "          Dropout-11             [-1, 32, 7, 7]               0\n",
            "           Conv2d-12             [-1, 64, 7, 7]          18,496\n",
            "             ReLU-13             [-1, 64, 7, 7]               0\n",
            "      BatchNorm2d-14             [-1, 64, 7, 7]             128\n",
            "           Conv2d-15             [-1, 64, 7, 7]          36,928\n",
            "             ReLU-16             [-1, 64, 7, 7]               0\n",
            "      BatchNorm2d-17             [-1, 64, 7, 7]             128\n",
            "           Conv2d-18             [-1, 64, 4, 4]          36,928\n",
            "             ReLU-19             [-1, 64, 4, 4]               0\n",
            "      BatchNorm2d-20             [-1, 64, 4, 4]             128\n",
            "        MaxPool2d-21             [-1, 64, 2, 2]               0\n",
            "          Dropout-22             [-1, 64, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]          73,856\n",
            "             ReLU-24            [-1, 128, 2, 2]               0\n",
            "      BatchNorm2d-25            [-1, 128, 2, 2]             256\n",
            "        MaxPool2d-26            [-1, 128, 1, 1]               0\n",
            "          Dropout-27            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-28            [-1, 128, 1, 1]               0\n",
            "           Linear-29                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 187,146\n",
            "Trainable params: 187,146\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.50\n",
            "Params size (MB): 0.71\n",
            "Estimated Total Size (MB): 2.22\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQHNVROHvwhw"
      },
      "source": [
        "## Preparing the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d47581-fe25-4164-a77a-da04dfcc5ae3"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.RandomRotation((-7.0, 7.0)),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ]))\n",
        "\n",
        "test = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ]))\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, **kwargs)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCmSK5cSv5nh"
      },
      "source": [
        "## Training and Testing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    epoch_loss=0\n",
        "    correct = 0\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        pbar.set_description(desc= f'epoch={epoch} Loss={loss.item()} batch_id={batch_idx:05d}')\n",
        "\n",
        "\n",
        "    train_loss = epoch_loss / len(train_loader.dataset)\n",
        "    train_acc=100.*correct/len(train_loader.dataset)\n",
        "    return train_loss,train_acc\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_acc=100. * correct / len(test_loader.dataset)\n",
        "    return test_loss,test_acc"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72086806-a819-4e88-be3f-f23f7fbe584b"
      },
      "source": [
        "# move the model to the specified device\n",
        "model = Net().to(device)\n",
        "# use Stochastic Gradient Descent as the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "# set the number of epochs to train for\n",
        "num_epoch = 8\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.015,epochs=19,steps_per_epoch=len(train_loader))\n",
        "\n",
        "train_loss_values = []\n",
        "test_loss_values = []\n",
        "# run it for epoch number of times\n",
        "for epoch in range(1, num_epoch+1):\n",
        "    print('\\nEpoch {} : '.format(epoch))\n",
        "    # train the model\n",
        "    train_loss=train_loss=train(model, device, train_loader, optimizer, epoch)\n",
        "    test_loss=test(model, device, test_loader)\n",
        "    # test the model\n",
        "    train_loss_values.append(train_loss)\n",
        "    test_loss_values.append(test_loss)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=1 Loss=0.18975119292736053 batch_id=00468: 100%|██████████| 469/469 [00:28<00:00, 16.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0556, Accuracy: 9826/10000 (98.26%)\n",
            "\n",
            "\n",
            "Epoch 2 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=2 Loss=0.10612019896507263 batch_id=00468: 100%|██████████| 469/469 [00:28<00:00, 16.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0317, Accuracy: 9907/10000 (99.07%)\n",
            "\n",
            "\n",
            "Epoch 3 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=3 Loss=0.06252444535493851 batch_id=00468: 100%|██████████| 469/469 [00:28<00:00, 16.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0275, Accuracy: 9914/10000 (99.14%)\n",
            "\n",
            "\n",
            "Epoch 4 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=4 Loss=0.021639101207256317 batch_id=00468: 100%|██████████| 469/469 [00:28<00:00, 16.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0241, Accuracy: 9922/10000 (99.22%)\n",
            "\n",
            "\n",
            "Epoch 5 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=5 Loss=0.07773462682962418 batch_id=00468: 100%|██████████| 469/469 [00:28<00:00, 16.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0216, Accuracy: 9935/10000 (99.35%)\n",
            "\n",
            "\n",
            "Epoch 6 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=6 Loss=0.030590320006012917 batch_id=00468: 100%|██████████| 469/469 [00:27<00:00, 16.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0202, Accuracy: 9936/10000 (99.36%)\n",
            "\n",
            "\n",
            "Epoch 7 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=7 Loss=0.06642211228609085 batch_id=00468: 100%|██████████| 469/469 [00:28<00:00, 16.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0162, Accuracy: 9949/10000 (99.49%)\n",
            "\n",
            "\n",
            "Epoch 8 : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=8 Loss=0.03770328685641289 batch_id=00468: 100%|██████████| 469/469 [00:28<00:00, 16.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0177, Accuracy: 9940/10000 (99.40%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}