{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2-5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOME3CHcMEiJKEdPLsdey+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuvaraj0001/EVA7_Assignments/blob/main/Session%202.5/Assignment_2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaDJsiskHZjs"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NufhzHuIHe_3"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh1AsTWoTwAD"
      },
      "source": [
        "## Building the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNhU1_5_JU7F"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)    # input-28x28  Output-28x28   RF-3x3\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)   # input-28x28  Output-28x28   RF-5x5\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)                # input-28x28  Output-14x14   RF-10x10\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)  # input-14x14  Output-14x14   RF-12x12\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1) # input-14x14  Output-14x14   RF-14x14\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)                # input-14x14  Output-7x7     RF-28x28\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3)            # input-7x7    Output-5x5     RF-30x30\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3)           # input-5x5    Output-3x3     RF-32x32\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3)            # input-3x3    Output-1x1     RF-34x34\n",
        "\n",
        "        self.fc1 = nn.Linear(10+10, 128)               # Concatenate two inputs           \n",
        "        self.fc2 = nn.Linear(128, 19)\n",
        "\n",
        "\n",
        "    def forward(self, image, randomNumber):\n",
        "        # 1st Step\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(image))))) # 1st convolutional layer followed by relu given to 2nd convolutional layer and output passed to maxpooling\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))     # 3rd convolutional layer followed by relu given to 4th convolutional layer and output passed to maxpooling\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))                 # Result from above passed to two sets of convolution and relu activation operation\n",
        "        x = self.conv7(x)                                             # The output from 6th convolutional layer is passed to a 7th convoluional linear layer\n",
        "        x = x.view(-1, 10)                                            # The flatten's all of the tensor's elements into a single dimension.\n",
        "\n",
        "        # 2nd Step\n",
        "        x1 = torch.cat((x, randomNumber), dim=1)                      # concatenate second input (random number) to the output from flattening\n",
        "        x1 = F.relu(self.fc1(x1))                                     # train network on dense layer\n",
        "        x1 = self.fc2(x1)                                           \n",
        "\n",
        "        # Model returns two outputs:\n",
        "        # 1. the \"number\" that was represented by the MNIST image \n",
        "        # 2. the \"sum\" of this number with the random number and the input image to the network\n",
        "        return F.log_softmax(x, dim = 1), F.log_softmax(x1, dim = 1)\n",
        "  "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayosj4v7xo85",
        "outputId": "4f5508b6-879b-401f-b234-059445925442"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv7): Conv2d(1024, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=20, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=19, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGJq9MDbAFcd"
      },
      "source": [
        "## Random number dataset generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzZSw6dCANxt"
      },
      "source": [
        "class Random_Num_Dataset(Dataset):\n",
        "  def __init__(self, MNISTDataset):\n",
        "    self.MNISTDataset = MNISTDataset\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image = self.MNISTDataset[index][0]\n",
        "    label = self.MNISTDataset[index][1]\n",
        "    randomNo = random.randint(0,9)\n",
        "\n",
        "    #Create one hot encoding for random number \n",
        "    one_hotrandomNo = torch.nn.functional.one_hot(torch.arange(0, 10))\n",
        "\n",
        "    #add actual label and random number\n",
        "    sum = label + randomNo\n",
        "    return image, label, one_hotrandomNo[randomNo], sum\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.MNISTDataset)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qFKWJ2CT2il"
      },
      "source": [
        "## Preparing the MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilbv1Qd17pIz"
      },
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "\n",
        "# Load and transform data\n",
        "MNIST_train = torchvision.datasets.MNIST('/tmp', train=True, download=True, transform=train_transforms)\n",
        "MNIST_trainset, MNIST_valset = torch.utils.data.random_split(MNIST_train, [55000, 5000])\n",
        "MNIST_testset = torchvision.datasets.MNIST('/tmp', train=False, download=True, transform=test_transforms)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R54yR6MMBnGv"
      },
      "source": [
        "# Load and transform data\n",
        "train_dataset = Random_Num_Dataset(MNIST_trainset)\n",
        "val_dataset = Random_Num_Dataset(MNIST_valset)\n",
        "test_dataset = Random_Num_Dataset(MNIST_testset)\n",
        "train_loader = DataLoader(train_dataset,batch_size=128,shuffle=True)\n",
        "val_loader = DataLoader(val_dataset,batch_size=128)\n",
        "test_loader = DataLoader(test_dataset,batch_size=128)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9GHP_iKUG_N"
      },
      "source": [
        "## Training and Testing Funtions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Mfaru2n7Le"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for batch_idx, (data, target, random_number, sum) in enumerate(train_loader):\n",
        "        target = target.type(torch.LongTensor)\n",
        "        sum = sum.type(torch.LongTensor)\n",
        "        data, target, sum = data.to(device), target.to(device), sum.to(device)\n",
        "        # zero out the gradients\n",
        "        optimizer.zero_grad()\n",
        "        output, sum_output = model(data,random_number.to(device))\n",
        "        # compute the loss \n",
        "        mnist_loss = F.nll_loss(output, target)\n",
        "        sum_loss = F.nll_loss(sum_output, sum)\n",
        "        loss= mnist_loss + sum_loss\n",
        "        epoch_loss += loss.item()\n",
        "        # compute the backward gradients\n",
        "        loss.backward()\n",
        "        # update the optimizer params\n",
        "        optimizer.step()\n",
        "    print('Train set: loss: {:.4f}'.format(loss.item()))\n",
        "\n",
        "    train_loss = epoch_loss / len(train_loader)\n",
        "    return train_loss\n",
        "\n",
        "def test(model, device, test_loader,test_type='Val'):\n",
        "    model.eval()\n",
        "    test_loss = 0   \n",
        "    correct_mnist = 0\n",
        "    correct_sum = 0\n",
        "    with torch.no_grad():\n",
        "        for (data, target, random_number, sum) in test_loader:\n",
        "            target = target.type(torch.LongTensor)\n",
        "            sum = sum.type(torch.LongTensor)\n",
        "            data, target,sum = data.to(device), target.to(device), sum.to(device)\n",
        "            output, sum_output = model(data,random_number.to(device))\n",
        "            test_loss += (F.nll_loss(output, target, reduction='sum').item() + F.nll_loss(sum_output, sum, reduction='sum').item())/2\n",
        "            mnist_pred = output.argmax(dim=1, keepdim=True)\n",
        "            sum_pred = sum_output.argmax(dim=1, keepdim=True)\n",
        "            correct_mnist += mnist_pred.eq(target.view_as(mnist_pred)).sum().item()\n",
        "            correct_sum += sum_pred.eq(sum.view_as(sum_pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'{test_type} set: loss: {test_loss:.3f}, MNist Accuracy:{100. * correct_mnist/len(test_loader.dataset)}, Sum_Accuracy:{100. * correct_sum/len(test_loader.dataset)}')\n",
        "    return test_loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3Ld1M3MphC1",
        "outputId": "26f49f42-4518-45c3-d574-fb1157042291"
      },
      "source": [
        "# move the model to the specified device\n",
        "model = Net().to(device)\n",
        "# use Stochastic Gradient Descent as the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "# set the number of epochs to train for\n",
        "num_epoch = 5\n",
        "\n",
        "train_loss_values = []\n",
        "valid_loss_values = []\n",
        "# run it for epoch number of times\n",
        "for epoch in range(1, num_epoch+1):\n",
        "    print('\\nEpoch {} : '.format(epoch))\n",
        "    # train the model\n",
        "    train_loss=train_loss=train(model, device, train_loader, optimizer, epoch)\n",
        "    valid_loss=test(model, device, val_loader)\n",
        "    # test the model\n",
        "    train_loss_values.append(train_loss)\n",
        "    valid_loss_values.append(valid_loss)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 : \n",
            "Train set: loss: 2.0403\n",
            "Val set: loss: 1.040, MNist Accuracy:98.26, Sum_Accuracy:35.22\n",
            "\n",
            "Epoch 2 : \n",
            "Train set: loss: 1.2970\n",
            "Val set: loss: 0.572, MNist Accuracy:98.76, Sum_Accuracy:89.2\n",
            "\n",
            "Epoch 3 : \n",
            "Train set: loss: 0.3346\n",
            "Val set: loss: 0.187, MNist Accuracy:98.92, Sum_Accuracy:98.52\n",
            "\n",
            "Epoch 4 : \n",
            "Train set: loss: 0.1684\n",
            "Val set: loss: 0.105, MNist Accuracy:98.88, Sum_Accuracy:98.62\n",
            "\n",
            "Epoch 5 : \n",
            "Train set: loss: 0.0975\n",
            "Val set: loss: 0.065, MNist Accuracy:99.14, Sum_Accuracy:99.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_FguabBvrZG"
      },
      "source": [
        "# Demo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEiXLXoJsf1S"
      },
      "source": [
        "def demo_prediction(image,rnum):\n",
        "\n",
        "    rnum_ohe = torch.zeros(11).long()\n",
        "    rnum_ohe[rnum] = 1\n",
        "    rnum_ohe = rnum_ohe[:-1]\n",
        "\n",
        "    output_1,output_2 = model(image.unsqueeze(dim=1).to(device),rnum_ohe.reshape(-1,10).to(device))\n",
        "\n",
        "    mnist_pred = output_1.argmax(dim=1, keepdim=True)\n",
        "    sum_pred = output_2.argmax(dim=1, keepdim=True)\n",
        "    plt.imshow(image.cpu().numpy().squeeze(),cmap='gray')\n",
        "    print('Mnist Prediction:',mnist_pred.item())\n",
        "    print('Random Number Generated:',rnum)\n",
        "    print('Sum:',mnist_pred.item()+rnum)\n",
        "    return mnist_pred, sum_pred"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "mHFYfz5Ks7jq",
        "outputId": "c844f437-8580-45a8-eb55-21e185a54ee8"
      },
      "source": [
        "image,_,_,_ = test_dataset[random.randint(1,10000)]\n",
        "rnum=random.randint(0,9)\n",
        "mnist_pred, sum_pred =demo_prediction(image,rnum)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mnist Prediction: 9\n",
            "Random Number Generated: 6\n",
            "Sum: 15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}